"""
ä¸ªè‚¡èµ„é‡‘æµå‘çˆ¬è™«ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºé‡æ„åçš„StockCapitalFlowScraperçš„å„ç§ç”¨æ³•
"""

import sys
import os
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from eastmoney_scraper import StockCapitalFlowScraper, MarketType, StockCapitalFlowMonitor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'stock_capital_flow_{datetime.now().strftime("%Y%m%d")}.log')
    ]
)

def example_single_scrape():
    """ç¤ºä¾‹1: å•æ¬¡çˆ¬å–å…¨å¸‚åœºæ•°æ®"""
    print("=" * 80)
    print("ç¤ºä¾‹1: å•æ¬¡çˆ¬å–å…¨å¸‚åœºä¸ªè‚¡èµ„é‡‘æµå‘æ•°æ®")
    print("=" * 80)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆå…¨å¸‚åœºï¼‰
    scraper = StockCapitalFlowScraper(market_type=MarketType.ALL, output_dir="output")
    
    # æ‰§è¡Œä¸€æ¬¡çˆ¬å–ï¼Œè·å–å‰5é¡µæ•°æ®ï¼Œä¿å­˜ä¸ºCSV
    df, filepath = scraper.run_once(max_pages=5, save_format='csv')
    
    if not df.empty:
        print(f"âœ… æˆåŠŸçˆ¬å– {len(df)} æ¡æ•°æ®")
        print(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        
        # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
        summary = scraper.analyze_market_summary(df)
        print(f"\nğŸ“Š å¸‚åœºæ¦‚å†µ:")
        for key, value in summary.items():
            print(f"   {key}: {value}")
        
        # æ˜¾ç¤ºä¸»åŠ›å‡€æµå…¥å‰10å
        top_inflow = scraper.get_top_inflow_stocks(df, 10)
        print(f"\nğŸ”¥ ä¸»åŠ›å‡€æµå…¥å‰10å:")
        print(top_inflow[['è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'æœ€æ–°ä»·', 'æ¶¨è·Œå¹…', 'ä¸»åŠ›å‡€æµå…¥', 'ä¸»åŠ›å‡€æµå…¥å æ¯”']])
        
    else:
        print("âŒ æœªè·å–åˆ°æ•°æ®")


def example_market_specific_scrape():
    """ç¤ºä¾‹2: çˆ¬å–ç‰¹å®šå¸‚åœºæ•°æ®"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹2: çˆ¬å–åˆ›ä¸šæ¿ä¸ªè‚¡èµ„é‡‘æµå‘æ•°æ®")
    print("=" * 80)
    
    # åˆ›å»ºåˆ›ä¸šæ¿çˆ¬è™«å®ä¾‹
    scraper = StockCapitalFlowScraper(
        market_type=MarketType.GEM,
        output_dir="output"
    )
    
    # çˆ¬å–æ•°æ®å¹¶ä¿å­˜ä¸ºJSONæ ¼å¼
    df, filepath = scraper.run_once(max_pages=3, save_format='json')
    
    if not df.empty:
        print(f"âœ… æˆåŠŸçˆ¬å–åˆ›ä¸šæ¿ {len(df)} æ¡æ•°æ®")
        print(f"ğŸ“ JSONæ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
        
        # åˆ†æåˆ›ä¸šæ¿å¸‚åœºæƒ…å†µ
        summary = scraper.analyze_market_summary(df)
        print(f"\nğŸ“Š åˆ›ä¸šæ¿å¸‚åœºæ¦‚å†µ:")
        print(f"   æ€»è‚¡ç¥¨æ•°: {summary.get('æ€»è‚¡ç¥¨æ•°', 0)}")
        print(f"   ä¸»åŠ›å‡€æµå…¥è‚¡ç¥¨æ•°: {summary.get('ä¸»åŠ›å‡€æµå…¥è‚¡ç¥¨æ•°', 0)}")
        print(f"   å¸‚åœºä¸»åŠ›å‡€æµå…¥æ€»é¢: {summary.get('å¸‚åœºä¸»åŠ›å‡€æµå…¥æ€»é¢(ä¸‡å…ƒ)', 0)} ä¸‡å…ƒ")
        print(f"   ä¸Šæ¶¨è‚¡ç¥¨æ•°: {summary.get('ä¸Šæ¶¨è‚¡ç¥¨æ•°', 0)}")
        
    else:
        print("âŒ æœªè·å–åˆ°åˆ›ä¸šæ¿æ•°æ®")


def example_scheduled_scraping():
    """ç¤ºä¾‹3: å®šæ—¶çˆ¬å–ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼Œè¿è¡Œ30ç§’ååœæ­¢ï¼‰"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹3: å®šæ—¶çˆ¬å–æ¼”ç¤ºï¼ˆè¿è¡Œ30ç§’ï¼‰")
    print("=" * 80)
    
    # åˆ›å»ºç§‘åˆ›æ¿çˆ¬è™«å®ä¾‹
    scraper = StockCapitalFlowScraper(
        market_type=MarketType.STAR,
        output_dir="output"
    )
    
    print("å¼€å§‹å®šæ—¶çˆ¬å–ç§‘åˆ›æ¿æ•°æ®ï¼Œæ¯20ç§’çˆ¬å–ä¸€æ¬¡...")
    print("(æ¼”ç¤ºæ¨¡å¼ï¼Œ30ç§’åè‡ªåŠ¨åœæ­¢)")
    
    import threading
    import time
    
    # å¯åŠ¨å®šæ—¶çˆ¬å–ï¼ˆåœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œï¼‰
    def run_scraping():
        scraper.start_scheduled_scraping(interval_seconds=20, max_pages=2, save_format='both')
    
    scrape_thread = threading.Thread(target=run_scraping, daemon=True)
    scrape_thread.start()
    
    # ç­‰å¾…30ç§’ååœæ­¢
    time.sleep(30)
    scraper.stop()
    print("âœ… å®šæ—¶çˆ¬å–æ¼”ç¤ºç»“æŸ")


def example_monitoring():
    """ç¤ºä¾‹4: å®æ—¶ç›‘æ§ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼Œè¿è¡Œ1åˆ†é’Ÿååœæ­¢ï¼‰"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹4: å®æ—¶ç›‘æ§æ¼”ç¤ºï¼ˆè¿è¡Œ1åˆ†é’Ÿï¼‰")
    print("=" * 80)
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = StockCapitalFlowMonitor(
        market_type=MarketType.MAIN_BOARD,
        output_dir="output"
    )
    
    print("å¼€å§‹å®æ—¶ç›‘æ§ä¸»æ¿èµ„é‡‘æµå‘...")
    print("(æ¼”ç¤ºæ¨¡å¼ï¼Œ1åˆ†é’Ÿåè‡ªåŠ¨åœæ­¢)")
    
    import threading
    import time
    
    # å¯åŠ¨ç›‘æ§ï¼ˆåœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œï¼‰
    def run_monitoring():
        monitor.start_monitoring(
            scrape_interval=30,  # 30ç§’çˆ¬å–ä¸€æ¬¡
            display_interval=15,  # 15ç§’åˆ·æ–°æ˜¾ç¤ºä¸€æ¬¡
            max_pages=2,
            save_format='csv'
        )
    
    monitor_thread = threading.Thread(target=run_monitoring, daemon=True)
    monitor_thread.start()
    
    # ç­‰å¾…60ç§’ååœæ­¢
    time.sleep(60)
    monitor.stop_monitoring()
    print("âœ… å®æ—¶ç›‘æ§æ¼”ç¤ºç»“æŸ")


def example_data_analysis():
    """ç¤ºä¾‹5: æ•°æ®åˆ†æåŠŸèƒ½æ¼”ç¤º"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹5: æ•°æ®åˆ†æåŠŸèƒ½æ¼”ç¤º")
    print("=" * 80)
    
    # å…ˆçˆ¬å–ä¸€äº›æ•°æ®
    scraper = StockCapitalFlowScraper(market_type=MarketType.ALL, output_dir="output")
    df, _ = scraper.run_once(max_pages=3, save_format='csv')
    
    if df.empty:
        print("âŒ æ— æ³•è·å–æ•°æ®è¿›è¡Œåˆ†æ")
        return
    
    print(f"ğŸ“Š åŸºäº {len(df)} æ¡æ•°æ®è¿›è¡Œåˆ†æ:")
    
    # è·å–ä¸»åŠ›å‡€æµå…¥æœ€å¤šçš„è‚¡ç¥¨
    top_inflow = scraper.get_top_inflow_stocks(df, 5)
    print(f"\nğŸ”¥ ä¸»åŠ›å‡€æµå…¥TOP5:")
    for _, row in top_inflow.iterrows():
        print(f"   {row['è‚¡ç¥¨åç§°']}({row['è‚¡ç¥¨ä»£ç ']}): {row['ä¸»åŠ›å‡€æµå…¥']:.2f}ä¸‡å…ƒ ({row['ä¸»åŠ›å‡€æµå…¥å æ¯”']:.2f}%)")
    
    # è·å–ä¸»åŠ›å‡€æµå‡ºæœ€å¤šçš„è‚¡ç¥¨
    top_outflow = scraper.get_top_outflow_stocks(df, 5)
    print(f"\nâ„ï¸ ä¸»åŠ›å‡€æµå‡ºTOP5:")
    for _, row in top_outflow.iterrows():
        print(f"   {row['è‚¡ç¥¨åç§°']}({row['è‚¡ç¥¨ä»£ç ']}): {row['ä¸»åŠ›å‡€æµå…¥']:.2f}ä¸‡å…ƒ ({row['ä¸»åŠ›å‡€æµå…¥å æ¯”']:.2f}%)")
    
    # å¸‚åœºæ¦‚å†µåˆ†æ
    summary = scraper.analyze_market_summary(df)
    print(f"\nğŸ“ˆ å¸‚åœºæ¦‚å†µ:")
    print(f"   ä¸»åŠ›å‡€æµå…¥è‚¡ç¥¨æ•°: {summary.get('ä¸»åŠ›å‡€æµå…¥è‚¡ç¥¨æ•°', 0)} / {summary.get('æ€»è‚¡ç¥¨æ•°', 0)}")
    print(f"   ä¸Šæ¶¨è‚¡ç¥¨æ•°: {summary.get('ä¸Šæ¶¨è‚¡ç¥¨æ•°', 0)} / {summary.get('æ€»è‚¡ç¥¨æ•°', 0)}")
    print(f"   å¸‚åœºæ€»å‡€æµå…¥: {summary.get('å¸‚åœºä¸»åŠ›å‡€æµå…¥æ€»é¢(ä¸‡å…ƒ)', 0):.2f} ä¸‡å…ƒ")


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸš€ ä¸ªè‚¡èµ„é‡‘æµå‘çˆ¬è™«ä½¿ç”¨ç¤ºä¾‹")
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # è¿è¡Œå„ä¸ªç¤ºä¾‹
        example_single_scrape()
        example_market_specific_scrape()
        example_data_analysis()
        
        # è¯¢é—®æ˜¯å¦è¿è¡Œå®šæ—¶ä»»åŠ¡ç¤ºä¾‹
        print("\n" + "=" * 80)
        response = input("æ˜¯å¦è¿è¡Œå®šæ—¶çˆ¬å–å’Œç›‘æ§ç¤ºä¾‹ï¼Ÿ(y/N): ").strip().lower()
        
        if response in ['y', 'yes']:
            example_scheduled_scraping()
            example_monitoring()
        else:
            print("â­ï¸ è·³è¿‡å®šæ—¶ä»»åŠ¡ç¤ºä¾‹")
        
        print("\n" + "=" * 80)
        print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
        print("ğŸ’¡ æç¤º: æŸ¥çœ‹ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶å’Œæ—¥å¿—ä»¥äº†è§£æ›´å¤šè¯¦æƒ…")
        print("ğŸ“ æ•°æ®æ–‡ä»¶ä¿å­˜åœ¨å„è‡ªçš„è¾“å‡ºç›®å½•ä¸­")
        
    except KeyboardInterrupt:
        print("\nâš¡ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œç¤ºä¾‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"â° ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


if __name__ == "__main__":
    main()